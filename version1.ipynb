{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import weaviate\n",
    "import numpy as np\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "auth_config = weaviate.AuthApiKey(api_key=\"cSS9XqOYrQ47rPmzCogrKYYm6rDhaZ7DJdCX\")\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url=\"https://test-s3sbv82a.weaviate.network\",\n",
    "    auth_client_secret=auth_config\n",
    ")\n",
    "paths = [\"test-dataset-1.md\", \"test-dataset-2.md\"]\n",
    "# 腾讯向量词\n",
    "def extract_tar_gz(file_path, extract_path):\n",
    "    # 解压缩文件\n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_path)\n",
    "\n",
    "# 检查特定文件是否存在\n",
    "specific_file_path = 'tencent-ailab-embedding-zh-d100-v0.2.0-s/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "tar_gz_path = 'tencent-ailab-embedding-zh-d100-v0.2.0-s.tar.gz'\n",
    "\n",
    "if not os.path.exists(specific_file_path):\n",
    "    print(\"Embedding file not found. Extracting now...\")\n",
    "    extract_tar_gz(tar_gz_path, '')\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(specific_file_path, binary=False)\n",
    "\n",
    "client.schema.delete_class(\"Article\")\n",
    "\n",
    "# 创建类\n",
    "article_class = {\n",
    "    \"class\": \"Article\",\n",
    "    \"description\": \"A class to represent articles\",\n",
    "    \"properties\": [\n",
    "        {\n",
    "            \"name\": \"title\",\n",
    "            \"description\": \"The title of the article\",\n",
    "            \"dataType\": [\"text\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"author\",\n",
    "            \"description\": \"The author of the article\",\n",
    "            \"dataType\": [\"text\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"content\",\n",
    "            \"description\": \"The content of the article\",\n",
    "            \"dataType\": [\"text\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 在Weaviate中添加类定义\n",
    "client.schema.create_class(article_class)\n",
    "# # get the schema\n",
    "# schema = client.schema.get()\n",
    "\n",
    "# # print the schema\n",
    "# print(json.dumps(schema, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished importing 45 segments.\n",
      "Imported 200 segments...\n",
      "Finished importing 364 segments.\n",
      "{\n",
      "    \"data\": {\n",
      "        \"Get\": {\n",
      "            \"Article\": [\n",
      "                {\n",
      "                    \"author\": \"杨川，新诤信知识产权\",\n",
      "                    \"content\": \"毫无疑问，技术资产是未来支撑具有硬核科创属性和持续高效发展的上市企业市值和商誉的核心资产\",\n",
      "                    \"title\": \"科技投行｜重新认识和界定技术资产\"\n",
      "                },\n",
      "                {\n",
      "                    \"author\": \"杨川，新诤信知识产权\",\n",
      "                    \"content\": \"无畏的执行者则是通过不懈的努力将愿景变成现实\",\n",
      "                    \"title\": \"创新驱动引擎：原理、模式与布局\"\n",
      "                },\n",
      "                {\n",
      "                    \"author\": \"杨川，新诤信知识产权\",\n",
      "                    \"content\": \"约翰·亨尼斯表示创新需要几类人：有远见的预言者、探索者、无畏的执行者。\",\n",
      "                    \"title\": \"创新驱动引擎：原理、模式与布局\"\n",
      "                },\n",
      "                {\n",
      "                    \"author\": \"杨川，新诤信知识产权\",\n",
      "                    \"content\": \"这种生态系统形成了彼此依赖、互补共生、协作繁荣的利益格局，谁也无法卡谁的脖子\",\n",
      "                    \"title\": \"创新驱动引擎：原理、模式与布局\"\n",
      "                },\n",
      "                {\n",
      "                    \"author\": \"杨川，新诤信知识产权\",\n",
      "                    \"content\": \"这是一个很容易被忽视的观察维度\",\n",
      "                    \"title\": \"创新驱动引擎：原理、模式与布局\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Initialize counter and interval for displaying progress\n",
    "counter = 0\n",
    "interval = 200\n",
    "\n",
    "def process_data(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    title_pattern = r\"# (.*?)\\n\"\n",
    "    author_pattern = r\">\\s*(.*?)\\n\"\n",
    "    content_pattern = r\"(?:>\\s*.*?\\n)([\\s\\S]*)\"\n",
    "    title = re.search(title_pattern, content).group(1)\n",
    "    author = re.search(author_pattern, content).group(1)\n",
    "    article_content = re.search(content_pattern, content).group(1)\n",
    "    punctuation = ' '\n",
    "    article_content = re.sub(f\"[{re.escape(punctuation)}]\", \"\", article_content)\n",
    "    article_content = article_content.replace(\"\\n\", \"\")\n",
    "    pattern = r'\\*\\*(.*?)\\*\\*|。|；'\n",
    "    segments = re.split(pattern, article_content)\n",
    "    # 过滤空字符串和None\n",
    "    segments = [seg.strip() for seg in segments if seg and seg.strip() not in ['##']]\n",
    "    return title, author, segments\n",
    "\n",
    "# 把文本分割后添加向量进数据库\n",
    "def add_article_to_batch(batch, title, author, segment, wv_from_text):\n",
    "    global counter\n",
    "    sentence_vector = [wv_from_text[word] for word in segment if word in wv_from_text.key_to_index]\n",
    "    if sentence_vector:\n",
    "        sentence_vector_mean = np.mean(sentence_vector, axis=0).tolist()\n",
    "    else:\n",
    "        sentence_vector_mean = []\n",
    "    properties = {\n",
    "        \"title\": title,\n",
    "        \"author\": author,\n",
    "        \"content\": segment.strip()\n",
    "    }\n",
    "    batch.add_data_object(properties, 'Article', vector=sentence_vector_mean)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % interval == 0:\n",
    "        print(f'Imported {counter} segments...')\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "for path in paths:\n",
    "    title, author, segments = process_data(path)\n",
    "    client.batch.configure(batch_size=100)\n",
    "\n",
    "    with client.batch as batch:\n",
    "        for segment in segments:  \n",
    "            add_article_to_batch(batch, title, author, segment, wv_from_text)\n",
    "\n",
    "    print(f'Finished importing {counter} segments.')\n",
    "\n",
    "def search_Query(keyword):\n",
    "    keyword = keyword\n",
    "    keyword_vector = [wv_from_text[word] for word in keyword if word in wv_from_text.key_to_index]\n",
    "    keyword_vector_mean = np.mean(keyword_vector, axis=0).tolist()\n",
    "\n",
    "    result = (\n",
    "        client.query\n",
    "        .get(\"Article\", [\"title\", \"content\",\"author\"])\n",
    "        .with_hybrid(keyword, alpha=0.5,properties=[\"content\"],vector=keyword_vector_mean)\n",
    "        .with_limit(5)\n",
    "        .do()\n",
    "    )\n",
    "    print(json.dumps(result, indent=4, ensure_ascii=False))\n",
    "\n",
    "search_Query('毫无疑问')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
